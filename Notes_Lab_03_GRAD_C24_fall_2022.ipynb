{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwa9a2mcV1qKRF2TxpiC0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hertie-School-Machine-Learning-F2022/Class_lab_03/blob/main/Notes_Lab_03_GRAD_C24_fall_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqWcir9N-sZB"
      },
      "outputs": [],
      "source": [
        "# Let's import a data set \n",
        "\n",
        "from sklearn import datasets\n",
        "\n",
        "# Let us use the parameter setting to make things easier for us: return_X_y = True, \n",
        "# return a dataframe please.\n",
        "\n",
        "X,y = datasets.load_breast_cancer(return_X_y = True, as_frame = True)\n",
        "\n",
        "# What is the data frame about?\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#examples-using-sklearn-datasets-load-breast-cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based in this, can we know what type of model we need? Classification or prediction? \n",
        "How many classes?"
      ],
      "metadata": {
        "id": "b06xUpD1q6uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take a quick look at X.\n",
        "# what other words can we use to call X?\n",
        "\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "CBaG_FEEjfKo",
        "outputId": "69c5b687-28c7-4620-c7c4-5d3ae038174a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "0          17.99         10.38          122.80     1001.0          0.11840   \n",
              "1          20.57         17.77          132.90     1326.0          0.08474   \n",
              "2          19.69         21.25          130.00     1203.0          0.10960   \n",
              "3          11.42         20.38           77.58      386.1          0.14250   \n",
              "4          20.29         14.34          135.10     1297.0          0.10030   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "564        21.56         22.39          142.00     1479.0          0.11100   \n",
              "565        20.13         28.25          131.20     1261.0          0.09780   \n",
              "566        16.60         28.08          108.30      858.1          0.08455   \n",
              "567        20.60         29.33          140.10     1265.0          0.11780   \n",
              "568         7.76         24.54           47.92      181.0          0.05263   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "0             0.27760         0.30010              0.14710         0.2419   \n",
              "1             0.07864         0.08690              0.07017         0.1812   \n",
              "2             0.15990         0.19740              0.12790         0.2069   \n",
              "3             0.28390         0.24140              0.10520         0.2597   \n",
              "4             0.13280         0.19800              0.10430         0.1809   \n",
              "..                ...             ...                  ...            ...   \n",
              "564           0.11590         0.24390              0.13890         0.1726   \n",
              "565           0.10340         0.14400              0.09791         0.1752   \n",
              "566           0.10230         0.09251              0.05302         0.1590   \n",
              "567           0.27700         0.35140              0.15200         0.2397   \n",
              "568           0.04362         0.00000              0.00000         0.1587   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "0                   0.07871  ...        25.380          17.33   \n",
              "1                   0.05667  ...        24.990          23.41   \n",
              "2                   0.05999  ...        23.570          25.53   \n",
              "3                   0.09744  ...        14.910          26.50   \n",
              "4                   0.05883  ...        22.540          16.67   \n",
              "..                      ...  ...           ...            ...   \n",
              "564                 0.05623  ...        25.450          26.40   \n",
              "565                 0.05533  ...        23.690          38.25   \n",
              "566                 0.05648  ...        18.980          34.12   \n",
              "567                 0.07016  ...        25.740          39.42   \n",
              "568                 0.05884  ...         9.456          30.37   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "0             184.60      2019.0           0.16220            0.66560   \n",
              "1             158.80      1956.0           0.12380            0.18660   \n",
              "2             152.50      1709.0           0.14440            0.42450   \n",
              "3              98.87       567.7           0.20980            0.86630   \n",
              "4             152.20      1575.0           0.13740            0.20500   \n",
              "..               ...         ...               ...                ...   \n",
              "564           166.10      2027.0           0.14100            0.21130   \n",
              "565           155.00      1731.0           0.11660            0.19220   \n",
              "566           126.70      1124.0           0.11390            0.30940   \n",
              "567           184.60      1821.0           0.16500            0.86810   \n",
              "568            59.16       268.6           0.08996            0.06444   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "0             0.7119                0.2654          0.4601   \n",
              "1             0.2416                0.1860          0.2750   \n",
              "2             0.4504                0.2430          0.3613   \n",
              "3             0.6869                0.2575          0.6638   \n",
              "4             0.4000                0.1625          0.2364   \n",
              "..               ...                   ...             ...   \n",
              "564           0.4107                0.2216          0.2060   \n",
              "565           0.3215                0.1628          0.2572   \n",
              "566           0.3403                0.1418          0.2218   \n",
              "567           0.9387                0.2650          0.4087   \n",
              "568           0.0000                0.0000          0.2871   \n",
              "\n",
              "     worst fractal dimension  \n",
              "0                    0.11890  \n",
              "1                    0.08902  \n",
              "2                    0.08758  \n",
              "3                    0.17300  \n",
              "4                    0.07678  \n",
              "..                       ...  \n",
              "564                  0.07115  \n",
              "565                  0.06637  \n",
              "566                  0.07820  \n",
              "567                  0.12400  \n",
              "568                  0.07039  \n",
              "\n",
              "[569 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-698b6768-b96f-4832-8349-91443267802e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>...</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>...</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>...</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>...</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>...</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>...</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>...</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>...</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>...</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>...</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-698b6768-b96f-4832-8349-91443267802e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-698b6768-b96f-4832-8349-91443267802e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-698b6768-b96f-4832-8349-91443267802e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And a look at y \n",
        "# what other words can we use to call y?\n",
        "\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faLgZUopkJl7",
        "outputId": "5a3e46e5-35c6-4aaf-8bd2-5e0eb256810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "564    0\n",
              "565    0\n",
              "566    0\n",
              "567    0\n",
              "568    1\n",
              "Name: target, Length: 569, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to seperate our data into training and testing data. "
      ],
      "metadata": {
        "id": "HnkAR0RnrPq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We need to set the name of the 4 objects that will be created\n",
        "# As arguments we add our X and our y, it will separate it into 4 \n",
        "# What are those 4 groups that we need? \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30)\n",
        "\n",
        "# We can move the size of the test size\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25)"
      ],
      "metadata": {
        "id": "oOWcfGKuqG7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets take a look at our data \n",
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ATrdZ52tqT3n",
        "outputId": "58a00dc8-1cd9-4a15-93b2-23262f698eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "109        11.34         21.26           72.48      396.5          0.08759   \n",
              "487        19.44         18.82          128.10     1167.0          0.10890   \n",
              "147        14.95         18.77           97.84      689.5          0.08138   \n",
              "276        11.33         14.16           71.79      396.6          0.09379   \n",
              "143        12.90         15.92           83.74      512.2          0.08677   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "304        11.46         18.16           73.59      403.1          0.08853   \n",
              "185        10.08         15.11           63.76      317.5          0.09267   \n",
              "438        13.85         19.60           88.68      592.6          0.08684   \n",
              "311        14.61         15.69           92.68      664.9          0.07618   \n",
              "262        17.29         22.13          114.40      947.8          0.08999   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "109           0.06575        0.051330             0.018990         0.1487   \n",
              "487           0.14480        0.225600             0.119400         0.1823   \n",
              "147           0.11670        0.090500             0.035620         0.1744   \n",
              "276           0.03872        0.001487             0.003333         0.1954   \n",
              "143           0.09509        0.048940             0.030880         0.1778   \n",
              "..                ...             ...                  ...            ...   \n",
              "304           0.07694        0.033440             0.015020         0.1411   \n",
              "185           0.04695        0.001597             0.002404         0.1703   \n",
              "438           0.06330        0.013420             0.022930         0.1555   \n",
              "311           0.03515        0.014470             0.018770         0.1632   \n",
              "262           0.12730        0.096970             0.075070         0.2108   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "109                 0.06529  ...         13.01          29.15   \n",
              "487                 0.06115  ...         23.96          30.39   \n",
              "147                 0.06493  ...         16.25          25.47   \n",
              "276                 0.05821  ...         12.20          18.99   \n",
              "143                 0.06235  ...         14.48          21.82   \n",
              "..                      ...  ...           ...            ...   \n",
              "304                 0.06243  ...         12.68          21.61   \n",
              "185                 0.06048  ...         11.87          21.18   \n",
              "438                 0.05673  ...         15.63          28.01   \n",
              "311                 0.05255  ...         16.46          21.75   \n",
              "262                 0.05464  ...         20.39          27.24   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "109            83.99       518.1            0.1699            0.21960   \n",
              "487           153.90      1740.0            0.1514            0.37250   \n",
              "147           107.10       809.7            0.0997            0.25210   \n",
              "276            77.37       458.0            0.1259            0.07348   \n",
              "143            97.17       643.8            0.1312            0.25480   \n",
              "..               ...         ...               ...                ...   \n",
              "304            82.69       489.8            0.1144            0.17890   \n",
              "185            75.39       437.0            0.1521            0.10190   \n",
              "438           100.90       749.1            0.1118            0.11410   \n",
              "311           103.70       840.8            0.1011            0.07087   \n",
              "262           137.90      1295.0            0.1134            0.28670   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "109         0.312000               0.08278          0.2829   \n",
              "487         0.593600               0.20600          0.3266   \n",
              "147         0.250000               0.08405          0.2852   \n",
              "276         0.004955               0.01111          0.2758   \n",
              "143         0.209000               0.10120          0.3549   \n",
              "..               ...                   ...             ...   \n",
              "304         0.122600               0.05509          0.2208   \n",
              "185         0.006920               0.01042          0.2933   \n",
              "438         0.047530               0.05890          0.2513   \n",
              "311         0.047460               0.05813          0.2530   \n",
              "262         0.229800               0.15280          0.3067   \n",
              "\n",
              "     worst fractal dimension  \n",
              "109                  0.08832  \n",
              "487                  0.09009  \n",
              "147                  0.09218  \n",
              "276                  0.06386  \n",
              "143                  0.08118  \n",
              "..                       ...  \n",
              "304                  0.07638  \n",
              "185                  0.07697  \n",
              "438                  0.06911  \n",
              "311                  0.05695  \n",
              "262                  0.07484  \n",
              "\n",
              "[426 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7b01d2ae-be7f-455e-94bb-1c6a9c3fd88c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>11.34</td>\n",
              "      <td>21.26</td>\n",
              "      <td>72.48</td>\n",
              "      <td>396.5</td>\n",
              "      <td>0.08759</td>\n",
              "      <td>0.06575</td>\n",
              "      <td>0.051330</td>\n",
              "      <td>0.018990</td>\n",
              "      <td>0.1487</td>\n",
              "      <td>0.06529</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01</td>\n",
              "      <td>29.15</td>\n",
              "      <td>83.99</td>\n",
              "      <td>518.1</td>\n",
              "      <td>0.1699</td>\n",
              "      <td>0.21960</td>\n",
              "      <td>0.312000</td>\n",
              "      <td>0.08278</td>\n",
              "      <td>0.2829</td>\n",
              "      <td>0.08832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>19.44</td>\n",
              "      <td>18.82</td>\n",
              "      <td>128.10</td>\n",
              "      <td>1167.0</td>\n",
              "      <td>0.10890</td>\n",
              "      <td>0.14480</td>\n",
              "      <td>0.225600</td>\n",
              "      <td>0.119400</td>\n",
              "      <td>0.1823</td>\n",
              "      <td>0.06115</td>\n",
              "      <td>...</td>\n",
              "      <td>23.96</td>\n",
              "      <td>30.39</td>\n",
              "      <td>153.90</td>\n",
              "      <td>1740.0</td>\n",
              "      <td>0.1514</td>\n",
              "      <td>0.37250</td>\n",
              "      <td>0.593600</td>\n",
              "      <td>0.20600</td>\n",
              "      <td>0.3266</td>\n",
              "      <td>0.09009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>14.95</td>\n",
              "      <td>18.77</td>\n",
              "      <td>97.84</td>\n",
              "      <td>689.5</td>\n",
              "      <td>0.08138</td>\n",
              "      <td>0.11670</td>\n",
              "      <td>0.090500</td>\n",
              "      <td>0.035620</td>\n",
              "      <td>0.1744</td>\n",
              "      <td>0.06493</td>\n",
              "      <td>...</td>\n",
              "      <td>16.25</td>\n",
              "      <td>25.47</td>\n",
              "      <td>107.10</td>\n",
              "      <td>809.7</td>\n",
              "      <td>0.0997</td>\n",
              "      <td>0.25210</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.08405</td>\n",
              "      <td>0.2852</td>\n",
              "      <td>0.09218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>11.33</td>\n",
              "      <td>14.16</td>\n",
              "      <td>71.79</td>\n",
              "      <td>396.6</td>\n",
              "      <td>0.09379</td>\n",
              "      <td>0.03872</td>\n",
              "      <td>0.001487</td>\n",
              "      <td>0.003333</td>\n",
              "      <td>0.1954</td>\n",
              "      <td>0.05821</td>\n",
              "      <td>...</td>\n",
              "      <td>12.20</td>\n",
              "      <td>18.99</td>\n",
              "      <td>77.37</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.1259</td>\n",
              "      <td>0.07348</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>0.01111</td>\n",
              "      <td>0.2758</td>\n",
              "      <td>0.06386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>12.90</td>\n",
              "      <td>15.92</td>\n",
              "      <td>83.74</td>\n",
              "      <td>512.2</td>\n",
              "      <td>0.08677</td>\n",
              "      <td>0.09509</td>\n",
              "      <td>0.048940</td>\n",
              "      <td>0.030880</td>\n",
              "      <td>0.1778</td>\n",
              "      <td>0.06235</td>\n",
              "      <td>...</td>\n",
              "      <td>14.48</td>\n",
              "      <td>21.82</td>\n",
              "      <td>97.17</td>\n",
              "      <td>643.8</td>\n",
              "      <td>0.1312</td>\n",
              "      <td>0.25480</td>\n",
              "      <td>0.209000</td>\n",
              "      <td>0.10120</td>\n",
              "      <td>0.3549</td>\n",
              "      <td>0.08118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>11.46</td>\n",
              "      <td>18.16</td>\n",
              "      <td>73.59</td>\n",
              "      <td>403.1</td>\n",
              "      <td>0.08853</td>\n",
              "      <td>0.07694</td>\n",
              "      <td>0.033440</td>\n",
              "      <td>0.015020</td>\n",
              "      <td>0.1411</td>\n",
              "      <td>0.06243</td>\n",
              "      <td>...</td>\n",
              "      <td>12.68</td>\n",
              "      <td>21.61</td>\n",
              "      <td>82.69</td>\n",
              "      <td>489.8</td>\n",
              "      <td>0.1144</td>\n",
              "      <td>0.17890</td>\n",
              "      <td>0.122600</td>\n",
              "      <td>0.05509</td>\n",
              "      <td>0.2208</td>\n",
              "      <td>0.07638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>10.08</td>\n",
              "      <td>15.11</td>\n",
              "      <td>63.76</td>\n",
              "      <td>317.5</td>\n",
              "      <td>0.09267</td>\n",
              "      <td>0.04695</td>\n",
              "      <td>0.001597</td>\n",
              "      <td>0.002404</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.06048</td>\n",
              "      <td>...</td>\n",
              "      <td>11.87</td>\n",
              "      <td>21.18</td>\n",
              "      <td>75.39</td>\n",
              "      <td>437.0</td>\n",
              "      <td>0.1521</td>\n",
              "      <td>0.10190</td>\n",
              "      <td>0.006920</td>\n",
              "      <td>0.01042</td>\n",
              "      <td>0.2933</td>\n",
              "      <td>0.07697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>13.85</td>\n",
              "      <td>19.60</td>\n",
              "      <td>88.68</td>\n",
              "      <td>592.6</td>\n",
              "      <td>0.08684</td>\n",
              "      <td>0.06330</td>\n",
              "      <td>0.013420</td>\n",
              "      <td>0.022930</td>\n",
              "      <td>0.1555</td>\n",
              "      <td>0.05673</td>\n",
              "      <td>...</td>\n",
              "      <td>15.63</td>\n",
              "      <td>28.01</td>\n",
              "      <td>100.90</td>\n",
              "      <td>749.1</td>\n",
              "      <td>0.1118</td>\n",
              "      <td>0.11410</td>\n",
              "      <td>0.047530</td>\n",
              "      <td>0.05890</td>\n",
              "      <td>0.2513</td>\n",
              "      <td>0.06911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>14.61</td>\n",
              "      <td>15.69</td>\n",
              "      <td>92.68</td>\n",
              "      <td>664.9</td>\n",
              "      <td>0.07618</td>\n",
              "      <td>0.03515</td>\n",
              "      <td>0.014470</td>\n",
              "      <td>0.018770</td>\n",
              "      <td>0.1632</td>\n",
              "      <td>0.05255</td>\n",
              "      <td>...</td>\n",
              "      <td>16.46</td>\n",
              "      <td>21.75</td>\n",
              "      <td>103.70</td>\n",
              "      <td>840.8</td>\n",
              "      <td>0.1011</td>\n",
              "      <td>0.07087</td>\n",
              "      <td>0.047460</td>\n",
              "      <td>0.05813</td>\n",
              "      <td>0.2530</td>\n",
              "      <td>0.05695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>17.29</td>\n",
              "      <td>22.13</td>\n",
              "      <td>114.40</td>\n",
              "      <td>947.8</td>\n",
              "      <td>0.08999</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.096970</td>\n",
              "      <td>0.075070</td>\n",
              "      <td>0.2108</td>\n",
              "      <td>0.05464</td>\n",
              "      <td>...</td>\n",
              "      <td>20.39</td>\n",
              "      <td>27.24</td>\n",
              "      <td>137.90</td>\n",
              "      <td>1295.0</td>\n",
              "      <td>0.1134</td>\n",
              "      <td>0.28670</td>\n",
              "      <td>0.229800</td>\n",
              "      <td>0.15280</td>\n",
              "      <td>0.3067</td>\n",
              "      <td>0.07484</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>426 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b01d2ae-be7f-455e-94bb-1c6a9c3fd88c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b01d2ae-be7f-455e-94bb-1c6a9c3fd88c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b01d2ae-be7f-455e-94bb-1c6a9c3fd88c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "UTbw4ypyqcnD",
        "outputId": "f92a5f7b-36aa-4631-9bb4-767b29e8e2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
              "170       12.320         12.39           78.85      464.1          0.10280   \n",
              "253       17.300         17.08          113.00      928.2          0.10080   \n",
              "442       13.780         15.79           88.37      585.9          0.08817   \n",
              "423       13.660         19.13           89.46      575.3          0.09057   \n",
              "21         9.504         12.44           60.34      273.9          0.10240   \n",
              "..           ...           ...             ...        ...              ...   \n",
              "242       11.300         18.19           73.93      389.4          0.09592   \n",
              "349       11.950         14.96           77.23      426.7          0.11580   \n",
              "265       20.730         31.12          135.70     1419.0          0.09469   \n",
              "64        12.680         23.84           82.69      499.0          0.11220   \n",
              "481       13.900         19.24           88.73      602.9          0.07991   \n",
              "\n",
              "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
              "170           0.06981         0.03987             0.037000         0.1959   \n",
              "253           0.10410         0.12660             0.083530         0.1813   \n",
              "442           0.06718         0.01055             0.009937         0.1405   \n",
              "423           0.11470         0.09657             0.048120         0.1848   \n",
              "21            0.06492         0.02956             0.020760         0.1815   \n",
              "..                ...             ...                  ...            ...   \n",
              "242           0.13250         0.15480             0.028540         0.2054   \n",
              "349           0.12060         0.01171             0.017870         0.2459   \n",
              "265           0.11430         0.13670             0.086460         0.1769   \n",
              "64            0.12620         0.11280             0.068730         0.1905   \n",
              "481           0.05326         0.02995             0.020700         0.1579   \n",
              "\n",
              "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
              "170                 0.05955  ...         13.50          15.64   \n",
              "253                 0.05613  ...         19.85          25.09   \n",
              "442                 0.05848  ...         15.27          17.50   \n",
              "423                 0.06181  ...         15.14          25.50   \n",
              "21                  0.06905  ...         10.23          15.66   \n",
              "..                      ...  ...           ...            ...   \n",
              "242                 0.07669  ...         12.58          27.96   \n",
              "349                 0.06581  ...         12.81          17.72   \n",
              "265                 0.05674  ...         32.49          47.16   \n",
              "64                  0.06590  ...         17.09          33.47   \n",
              "481                 0.05594  ...         16.41          26.42   \n",
              "\n",
              "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
              "170            86.97       549.1            0.1385             0.1266   \n",
              "253           130.90      1222.0            0.1416             0.2405   \n",
              "442            97.90       706.6            0.1072             0.1071   \n",
              "423           101.40       708.8            0.1147             0.3167   \n",
              "21             65.13       314.9            0.1324             0.1148   \n",
              "..               ...         ...               ...                ...   \n",
              "242            87.16       472.9            0.1347             0.4848   \n",
              "349            83.09       496.2            0.1293             0.1885   \n",
              "265           214.00      3432.0            0.1401             0.2644   \n",
              "64            111.80       888.3            0.1851             0.4061   \n",
              "481           104.40       830.5            0.1064             0.1415   \n",
              "\n",
              "     worst concavity  worst concave points  worst symmetry  \\\n",
              "170          0.12420               0.09391          0.2827   \n",
              "253          0.33780               0.18570          0.3138   \n",
              "442          0.03517               0.03312          0.1859   \n",
              "423          0.36600               0.14070          0.2744   \n",
              "21           0.08867               0.06227          0.2450   \n",
              "..               ...                   ...             ...   \n",
              "242          0.74360               0.12180          0.3308   \n",
              "349          0.03122               0.04766          0.3124   \n",
              "265          0.34420               0.16590          0.2868   \n",
              "64           0.40240               0.17160          0.3383   \n",
              "481          0.16730               0.08150          0.2356   \n",
              "\n",
              "     worst fractal dimension  \n",
              "170                  0.06771  \n",
              "253                  0.08113  \n",
              "442                  0.06810  \n",
              "423                  0.08839  \n",
              "21                   0.07773  \n",
              "..                       ...  \n",
              "242                  0.12970  \n",
              "349                  0.07590  \n",
              "265                  0.08218  \n",
              "64                   0.10310  \n",
              "481                  0.07603  \n",
              "\n",
              "[143 rows x 30 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95aa4e8f-c6b3-40ca-a1c4-c9741bede886\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>...</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>12.320</td>\n",
              "      <td>12.39</td>\n",
              "      <td>78.85</td>\n",
              "      <td>464.1</td>\n",
              "      <td>0.10280</td>\n",
              "      <td>0.06981</td>\n",
              "      <td>0.03987</td>\n",
              "      <td>0.037000</td>\n",
              "      <td>0.1959</td>\n",
              "      <td>0.05955</td>\n",
              "      <td>...</td>\n",
              "      <td>13.50</td>\n",
              "      <td>15.64</td>\n",
              "      <td>86.97</td>\n",
              "      <td>549.1</td>\n",
              "      <td>0.1385</td>\n",
              "      <td>0.1266</td>\n",
              "      <td>0.12420</td>\n",
              "      <td>0.09391</td>\n",
              "      <td>0.2827</td>\n",
              "      <td>0.06771</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>17.300</td>\n",
              "      <td>17.08</td>\n",
              "      <td>113.00</td>\n",
              "      <td>928.2</td>\n",
              "      <td>0.10080</td>\n",
              "      <td>0.10410</td>\n",
              "      <td>0.12660</td>\n",
              "      <td>0.083530</td>\n",
              "      <td>0.1813</td>\n",
              "      <td>0.05613</td>\n",
              "      <td>...</td>\n",
              "      <td>19.85</td>\n",
              "      <td>25.09</td>\n",
              "      <td>130.90</td>\n",
              "      <td>1222.0</td>\n",
              "      <td>0.1416</td>\n",
              "      <td>0.2405</td>\n",
              "      <td>0.33780</td>\n",
              "      <td>0.18570</td>\n",
              "      <td>0.3138</td>\n",
              "      <td>0.08113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>13.780</td>\n",
              "      <td>15.79</td>\n",
              "      <td>88.37</td>\n",
              "      <td>585.9</td>\n",
              "      <td>0.08817</td>\n",
              "      <td>0.06718</td>\n",
              "      <td>0.01055</td>\n",
              "      <td>0.009937</td>\n",
              "      <td>0.1405</td>\n",
              "      <td>0.05848</td>\n",
              "      <td>...</td>\n",
              "      <td>15.27</td>\n",
              "      <td>17.50</td>\n",
              "      <td>97.90</td>\n",
              "      <td>706.6</td>\n",
              "      <td>0.1072</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>0.03517</td>\n",
              "      <td>0.03312</td>\n",
              "      <td>0.1859</td>\n",
              "      <td>0.06810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>13.660</td>\n",
              "      <td>19.13</td>\n",
              "      <td>89.46</td>\n",
              "      <td>575.3</td>\n",
              "      <td>0.09057</td>\n",
              "      <td>0.11470</td>\n",
              "      <td>0.09657</td>\n",
              "      <td>0.048120</td>\n",
              "      <td>0.1848</td>\n",
              "      <td>0.06181</td>\n",
              "      <td>...</td>\n",
              "      <td>15.14</td>\n",
              "      <td>25.50</td>\n",
              "      <td>101.40</td>\n",
              "      <td>708.8</td>\n",
              "      <td>0.1147</td>\n",
              "      <td>0.3167</td>\n",
              "      <td>0.36600</td>\n",
              "      <td>0.14070</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.08839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>9.504</td>\n",
              "      <td>12.44</td>\n",
              "      <td>60.34</td>\n",
              "      <td>273.9</td>\n",
              "      <td>0.10240</td>\n",
              "      <td>0.06492</td>\n",
              "      <td>0.02956</td>\n",
              "      <td>0.020760</td>\n",
              "      <td>0.1815</td>\n",
              "      <td>0.06905</td>\n",
              "      <td>...</td>\n",
              "      <td>10.23</td>\n",
              "      <td>15.66</td>\n",
              "      <td>65.13</td>\n",
              "      <td>314.9</td>\n",
              "      <td>0.1324</td>\n",
              "      <td>0.1148</td>\n",
              "      <td>0.08867</td>\n",
              "      <td>0.06227</td>\n",
              "      <td>0.2450</td>\n",
              "      <td>0.07773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>11.300</td>\n",
              "      <td>18.19</td>\n",
              "      <td>73.93</td>\n",
              "      <td>389.4</td>\n",
              "      <td>0.09592</td>\n",
              "      <td>0.13250</td>\n",
              "      <td>0.15480</td>\n",
              "      <td>0.028540</td>\n",
              "      <td>0.2054</td>\n",
              "      <td>0.07669</td>\n",
              "      <td>...</td>\n",
              "      <td>12.58</td>\n",
              "      <td>27.96</td>\n",
              "      <td>87.16</td>\n",
              "      <td>472.9</td>\n",
              "      <td>0.1347</td>\n",
              "      <td>0.4848</td>\n",
              "      <td>0.74360</td>\n",
              "      <td>0.12180</td>\n",
              "      <td>0.3308</td>\n",
              "      <td>0.12970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>11.950</td>\n",
              "      <td>14.96</td>\n",
              "      <td>77.23</td>\n",
              "      <td>426.7</td>\n",
              "      <td>0.11580</td>\n",
              "      <td>0.12060</td>\n",
              "      <td>0.01171</td>\n",
              "      <td>0.017870</td>\n",
              "      <td>0.2459</td>\n",
              "      <td>0.06581</td>\n",
              "      <td>...</td>\n",
              "      <td>12.81</td>\n",
              "      <td>17.72</td>\n",
              "      <td>83.09</td>\n",
              "      <td>496.2</td>\n",
              "      <td>0.1293</td>\n",
              "      <td>0.1885</td>\n",
              "      <td>0.03122</td>\n",
              "      <td>0.04766</td>\n",
              "      <td>0.3124</td>\n",
              "      <td>0.07590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>20.730</td>\n",
              "      <td>31.12</td>\n",
              "      <td>135.70</td>\n",
              "      <td>1419.0</td>\n",
              "      <td>0.09469</td>\n",
              "      <td>0.11430</td>\n",
              "      <td>0.13670</td>\n",
              "      <td>0.086460</td>\n",
              "      <td>0.1769</td>\n",
              "      <td>0.05674</td>\n",
              "      <td>...</td>\n",
              "      <td>32.49</td>\n",
              "      <td>47.16</td>\n",
              "      <td>214.00</td>\n",
              "      <td>3432.0</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.2644</td>\n",
              "      <td>0.34420</td>\n",
              "      <td>0.16590</td>\n",
              "      <td>0.2868</td>\n",
              "      <td>0.08218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>12.680</td>\n",
              "      <td>23.84</td>\n",
              "      <td>82.69</td>\n",
              "      <td>499.0</td>\n",
              "      <td>0.11220</td>\n",
              "      <td>0.12620</td>\n",
              "      <td>0.11280</td>\n",
              "      <td>0.068730</td>\n",
              "      <td>0.1905</td>\n",
              "      <td>0.06590</td>\n",
              "      <td>...</td>\n",
              "      <td>17.09</td>\n",
              "      <td>33.47</td>\n",
              "      <td>111.80</td>\n",
              "      <td>888.3</td>\n",
              "      <td>0.1851</td>\n",
              "      <td>0.4061</td>\n",
              "      <td>0.40240</td>\n",
              "      <td>0.17160</td>\n",
              "      <td>0.3383</td>\n",
              "      <td>0.10310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>13.900</td>\n",
              "      <td>19.24</td>\n",
              "      <td>88.73</td>\n",
              "      <td>602.9</td>\n",
              "      <td>0.07991</td>\n",
              "      <td>0.05326</td>\n",
              "      <td>0.02995</td>\n",
              "      <td>0.020700</td>\n",
              "      <td>0.1579</td>\n",
              "      <td>0.05594</td>\n",
              "      <td>...</td>\n",
              "      <td>16.41</td>\n",
              "      <td>26.42</td>\n",
              "      <td>104.40</td>\n",
              "      <td>830.5</td>\n",
              "      <td>0.1064</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.16730</td>\n",
              "      <td>0.08150</td>\n",
              "      <td>0.2356</td>\n",
              "      <td>0.07603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143 rows × 30 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95aa4e8f-c6b3-40ca-a1c4-c9741bede886')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-95aa4e8f-c6b3-40ca-a1c4-c9741bede886 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-95aa4e8f-c6b3-40ca-a1c4-c9741bede886');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E751CwZcqeVC",
        "outputId": "49fa93d0-df09-44f2-f69b-8adf08edbfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109    1\n",
              "487    0\n",
              "147    1\n",
              "276    1\n",
              "143    1\n",
              "      ..\n",
              "304    1\n",
              "185    1\n",
              "438    1\n",
              "311    1\n",
              "262    0\n",
              "Name: target, Length: 426, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTCiFUGtqfpe",
        "outputId": "481b7f3f-ce1b-408c-ddd3-168506464a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170    1\n",
              "253    0\n",
              "442    1\n",
              "423    1\n",
              "21     1\n",
              "      ..\n",
              "242    1\n",
              "349    1\n",
              "265    0\n",
              "64     0\n",
              "481    1\n",
              "Name: target, Length: 143, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to now what have we done?\n",
        "Can we add this data to a classification model? \n",
        "What classification models come to mind?\n",
        "\n"
      ],
      "metadata": {
        "id": "U4yEH3QulFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's build a simple classification pipeline. First import pipeline\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Our data has different dimesions, so let's scale it. Import Scaler\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Now let's chose our model \n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Two step pipeline. First preprocess the data with the Standard Scaler,\n",
        "# then add the model, in this case a Logistic Regression \n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()),     # Step 1\n",
        "                 ('model', LogisticRegression()) # Step 2\n",
        "                 ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Pusfw_r4jG",
        "outputId": "524537a6-066b-42b7-a6c1-9db18e4ba54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('scaler', StandardScaler()), ('model', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Which data can we fit now? \n",
        "\n",
        "pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "tcVXRZ1GtSMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the next step? Prediction \n",
        "\n",
        "pred = pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "yd_lfzJotUCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the outcome of the .predict function?\n",
        "\n",
        "print(pred)\n",
        "\n",
        "# an array with what? "
      ],
      "metadata": {
        "id": "RD552QIttcYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics \n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, pred) \n",
        "\n",
        "cnf_matrix "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdtFBcCYtoom",
        "outputId": "66b4edc0-9c8e-402a-f861-5d275585b762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54,  2],\n",
              "       [ 2, 85]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import seaborn as sns \n",
        "\n",
        "\n",
        "\n",
        "class_names=[0,1] # name  of classes \n",
        "\n",
        "fig, ax = plt.subplots() \n",
        "\n",
        "tick_marks = np.arange(len(class_names)) \n",
        "\n",
        "plt.xticks(tick_marks, class_names) \n",
        "\n",
        "plt.yticks(tick_marks, class_names) \n",
        "\n",
        "# create heatmap \n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') \n",
        "\n",
        "ax.xaxis.set_label_position(\"top\") \n",
        "\n",
        "plt.tight_layout() \n",
        "\n",
        "plt.title('Confusion matrix', y=1.1) \n",
        "\n",
        "plt.ylabel('Actual label') \n",
        "\n",
        "plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "4FL_0kAG0wHp",
        "outputId": "859f44e2-3b85-45b2-97cf-604e27cd58c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted label')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy = (TP + TN) / (ALL)\n",
        "\n",
        "Precision = TP / (TP + FP)"
      ],
      "metadata": {
        "id": "49bRbmdP4S7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred)) \n",
        "print(\"Precision:\",metrics.precision_score(y_test, pred)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-lvc7J11GH0",
        "outputId": "69dcc2f8-04de-431a-f40f-7e942d57481c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.972027972027972\n",
            "Precision: 0.9770114942528736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We the problem with our model is that it has already seen the testing data. \n",
        "# And that generates leakage probelms. It's kind of like cheating\n",
        "\n",
        "# How can we make sure we have the best possible model without using our test data?\n",
        "\n",
        "# Cross Validation \n",
        "\n",
        "# The idea behind cross validation, is to generate pseudo testing and traing by \n",
        "# spliting the testing data into folds and getting our model to test and train on \n",
        "# unseen data, without having to intoduce our testing data.\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cv = KFold(n_splits=3, random_state=123, shuffle=True)\n",
        "\n",
        "scores = cross_val_score(pipe, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)"
      ],
      "metadata": {
        "id": "QWVtJ3A58d23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CIqXUJO85kt",
        "outputId": "5f87b9fb-c54a-42fd-f6e6-303c197d510f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9375    , 0.95833333, 0.95744681])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What else can we do to make our model better? \n",
        "# We know about standardscaler for preprocessing for example. \n",
        "# But there are other thing inside the logistic regression algorithm\n",
        "# that we can tweek to have the best possible model. While we tune\n",
        "# we use CV to test out the parameters.\n",
        "\n",
        "# what parameters can we modify?\n",
        "pipe.get_params()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66P-M5k85fx",
        "outputId": "6d80baac-3d5f-4f79-9c68-812d6b031ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'memory': None,\n",
              " 'steps': [('scaler', StandardScaler()), ('model', LogisticRegression())],\n",
              " 'verbose': False,\n",
              " 'scaler': StandardScaler(),\n",
              " 'model': LogisticRegression(),\n",
              " 'scaler__copy': True,\n",
              " 'scaler__with_mean': True,\n",
              " 'scaler__with_std': True,\n",
              " 'model__C': 1.0,\n",
              " 'model__class_weight': None,\n",
              " 'model__dual': False,\n",
              " 'model__fit_intercept': True,\n",
              " 'model__intercept_scaling': 1,\n",
              " 'model__l1_ratio': None,\n",
              " 'model__max_iter': 100,\n",
              " 'model__multi_class': 'auto',\n",
              " 'model__n_jobs': None,\n",
              " 'model__penalty': 'l2',\n",
              " 'model__random_state': None,\n",
              " 'model__solver': 'lbfgs',\n",
              " 'model__tol': 0.0001,\n",
              " 'model__verbose': 0,\n",
              " 'model__warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now use the GridSearchCV object to automatically \n",
        "# fit models and predict models with different parameters\n",
        "# while using cross validadtion. Then compare the results.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "mod = GridSearchCV(estimator=pipe,\n",
        "                 #using a dict object we add the things we want the model to try\n",
        "                 param_grid={\n",
        "                   'model__penalty': ['l1', 'l2'],          # Parameter penalty, this two options\n",
        "                   'model__solver': ['lbfgs', 'liblinear'] # Parameter solver, try this two options\n",
        "                 },\n",
        "                 cv=3) # and cross validate 3 times"
      ],
      "metadata": {
        "id": "S-CINeWE_-hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can fit the model now \n",
        "mod.fit(X_train, y_train);\n",
        "\n",
        "# and take a look at the results\n",
        "pd.DataFrame(mod.cv_results_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "ZnBFSJkfAS2J",
        "outputId": "5343c6f2-2180-4733-f331-d3f1fe9934a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "3 fits failed out of a total of 12.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "3 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.97183099 0.97652582 0.97887324]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
              "0       0.005033      0.002686         0.000000        0.000000   \n",
              "1       0.004908      0.000124         0.001763        0.000097   \n",
              "2       0.008972      0.000299         0.001789        0.000021   \n",
              "3       0.005501      0.000993         0.001918        0.000582   \n",
              "\n",
              "  param_model__penalty param_model__solver  \\\n",
              "0                   l1               lbfgs   \n",
              "1                   l1           liblinear   \n",
              "2                   l2               lbfgs   \n",
              "3                   l2           liblinear   \n",
              "\n",
              "                                              params  split0_test_score  \\\n",
              "0  {'model__penalty': 'l1', 'model__solver': 'lbf...                NaN   \n",
              "1  {'model__penalty': 'l1', 'model__solver': 'lib...           0.985915   \n",
              "2  {'model__penalty': 'l2', 'model__solver': 'lbf...           0.992958   \n",
              "3  {'model__penalty': 'l2', 'model__solver': 'lib...           0.992958   \n",
              "\n",
              "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
              "0                NaN                NaN              NaN             NaN   \n",
              "1           0.964789           0.964789         0.971831        0.009959   \n",
              "2           0.971831           0.964789         0.976526        0.011970   \n",
              "3           0.978873           0.964789         0.978873        0.011500   \n",
              "\n",
              "   rank_test_score  \n",
              "0                4  \n",
              "1                3  \n",
              "2                2  \n",
              "3                1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e026da5-afbf-4422-a5c0-5de3a5ed4d85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_model__penalty</th>\n",
              "      <th>param_model__solver</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.005033</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>l1</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>{'model__penalty': 'l1', 'model__solver': 'lbf...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.004908</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.001763</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>l1</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>{'model__penalty': 'l1', 'model__solver': 'lib...</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.971831</td>\n",
              "      <td>0.009959</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.008972</td>\n",
              "      <td>0.000299</td>\n",
              "      <td>0.001789</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>l2</td>\n",
              "      <td>lbfgs</td>\n",
              "      <td>{'model__penalty': 'l2', 'model__solver': 'lbf...</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.971831</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.976526</td>\n",
              "      <td>0.011970</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.005501</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>0.001918</td>\n",
              "      <td>0.000582</td>\n",
              "      <td>l2</td>\n",
              "      <td>liblinear</td>\n",
              "      <td>{'model__penalty': 'l2', 'model__solver': 'lib...</td>\n",
              "      <td>0.992958</td>\n",
              "      <td>0.978873</td>\n",
              "      <td>0.964789</td>\n",
              "      <td>0.978873</td>\n",
              "      <td>0.011500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e026da5-afbf-4422-a5c0-5de3a5ed4d85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e026da5-afbf-4422-a5c0-5de3a5ed4d85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e026da5-afbf-4422-a5c0-5de3a5ed4d85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We now see that the best parameters are 'model__penalty': 'l1', 'model__solver': 'liblinear'\n",
        "# So we create a pipeline with the specs we want \n",
        "\n",
        "pipe = Pipeline([('scaler', StandardScaler()),     # Step 1\n",
        "                 ('model', LogisticRegression(penalty = 'l1', solver = 'liblinear')) # Step 2\n",
        "                 ])\n",
        "\n",
        "# and fit and predict\n",
        "\n",
        "pred = pipe.fit(X_train, y_train).predict(X_test)"
      ],
      "metadata": {
        "id": "C4zyTnpaAcgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see how well it did\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, pred) \n",
        "\n",
        "print(cnf_matrix)\n",
        "\n",
        "# Pretty good! \n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred)) \n",
        "print(\"Precision:\",metrics.precision_score(y_test, pred)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RQHBKSTBI50",
        "outputId": "8f402aaf-ba45-4b70-891b-87f79aeb0a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[54  2]\n",
            " [ 2 85]]\n",
            "Accuracy: 0.972027972027972\n",
            "Precision: 0.9770114942528736\n"
          ]
        }
      ]
    }
  ]
}